{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import facenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import facenet\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from utils.image import resize_image_to_larger_dimension_and_pad\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spot check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256  #don't need equal to real image size, but this value should not small than this\n",
    "modeldir = '/home/caffe/facenet/sku_triplet_500k.pb' #change to your model dir\n",
    "image_name1 = '/datadrive/images/activelearning/ccna_add_train_reference_crop_test/1018120/tmp#110460.jpg' #change to your image name\n",
    "image_name2 = '/datadrive/images/activelearning/ccna_add_train_reference_crop_test/1018440/tmp#216495.jpg' #change to your image name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image1 = imageio.imread(image_name1, pilmode='RGB')\n",
    "plt.imshow(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image1 = facenet.prewhiten(image1)\n",
    "plt.imshow(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image2 = imageio.imread(image_name2, pilmode='RGB')\n",
    "plt.imshow(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image2 = facenet.prewhiten(image2)\n",
    "plt.imshow(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('setting up facenet embedding')\n",
    "tf.Graph().as_default()\n",
    "sess = tf.Session()\n",
    "facenet.load_model(modeldir)\n",
    "images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "embedding_size = embeddings.get_shape()[1]\n",
    "\n",
    "print('facenet embedding is generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_dist(emb_dir1, image_file1, emb_dir2, image_file2):\n",
    "    emb1 = load_emb(emb_dir1,image_file1)\n",
    "    emb2 = load_emb(emb_dir2,image_file2)\n",
    "    dist = np.sqrt(np.sum(np.square(emb1-emb2)))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_sku_embedding_mean(emb_dir,images_files):\n",
    "    emb_array = np.empty((0,embedding_size))\n",
    "    for idx,image_file in enumerate(images_files):\n",
    "        emb = load_emb(emb_dir,image_file)\n",
    "        emb_array = np.vstack((emb,emb_array))\n",
    "    return np.mean(emb_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_emb(image_file):\n",
    "    image = imageio.imread(image_file, pilmode='RGB')\n",
    "    image = facenet.prewhiten(image)\n",
    "    scaled_reshape = image.reshape(-1,image_size,image_size,3)\n",
    "    emb = np.zeros((1, embedding_size))\n",
    "    emb[0,:] = sess.run(embeddings, feed_dict={images_placeholder: scaled_reshape, phase_train_placeholder: False })[0]\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_emb(emb_dir,image_file,skip=True):\n",
    "    sku = image_file.split('/')[-2]\n",
    "    file_name = image_file.split('/')[-1]\n",
    "    if not os.path.exists(emb_dir + '/' + sku):\n",
    "        os.makedirs(emb_dir + '/' + sku)\n",
    "    emb_file = emb_dir + '/' + sku + '/' + file_name.split('.')[0] + '.npy'\n",
    "    if not (skip and os.path.exists(emb_file)):\n",
    "        emb = cal_emb(image_file)\n",
    "        np.save(emb_file,emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb(emb_dir, image_file):\n",
    "    sku = image_file.split('/')[-2]\n",
    "    file_name = image_file.split('/')[-1]\n",
    "    emb_file = emb_dir + '/' + sku + '/' + file_name.split('.')[0] + '.npy'\n",
    "    return np.load(emb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dir1 = '/datadrive/embs/ccna_embs1'\n",
    "emb_dir2 = '/datadrive/embs/ccna_embs2'\n",
    "save_emb(emb_dir1, image_name1)\n",
    "save_emb(emb_dir2, image_name2)\n",
    "\n",
    "images_files = [image_file for image_file in glob.glob('/datadrive/images/activelearning/ccna_add_train_reference_crop/1018438/' + '/*')]\n",
    "cal_sku_embedding_mean(emb_dir1, images_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_emb(emb_dir,image_name1)\n",
    "save_emb(emb_dir,image_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cal_dist(image_name1,image_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## given an image from train, calculate its distance from other images in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image = '/datadrive/images/activelearning/ccna_add_train_reference_crop/1018120/tmp#796199.jpg'\n",
    "train_dir = '/datadrive/images/activelearning/ccna_add_train_reference_crop'\n",
    "sample_num = 5\n",
    "random.seed(0)\n",
    "\n",
    "image_file_pool = []\n",
    "for sku_dir in glob.glob(train_dir + '/*'):\n",
    "    images_files = [image_file for image_file in glob.glob(sku_dir + '/*')]\n",
    "    image_file_pool.extend(random.sample(images_files,min(sample_num, len(images_files))))\n",
    "\n",
    "lst = []\n",
    "for image_file in image_file_pool:\n",
    "    dist = cal_dist(ref_image,image_file)\n",
    "    sku = image_file.split('/')[-2]\n",
    "    file_name = image_file.split('/')[-1]\n",
    "    lst.append([file_name, sku, dist])\n",
    "    print file_name,sku,dist\n",
    "\n",
    "df = pd.DataFrame(lst, columns=['file_name','sku','distance'])\n",
    "df.to_csv('train_train_check.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## given an image from test, calculate its distance from other images in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_image = '/datadrive/images/activelearning/ccna_add_train_reference_crop_test/1018120/tmp#110460.jpg'\n",
    "#query_emb = cal_emb(query_image)\n",
    "image_dir = '/datadrive/images/activelearning/ccna_add_train_reference_crop_test'\n",
    "emb_dir = '/datadrive/images/activelearning/ccna_add_train_reference_crop_test_emb'\n",
    "\n",
    "random.seed(0)\n",
    "sample_num = 5\n",
    "\n",
    "sku_mean_dict={}\n",
    "db_file_pool = []\n",
    "query_file_pool = []\n",
    "\n",
    "for sku_dir in glob.glob(image_dir + '/*'):\n",
    "    sku = sku_dir.split('/')[-1]\n",
    "    images_files = [image_file for image_file in glob.glob(sku_dir + '/*')]\n",
    "    image_files_sample = random.sample(images_files,min(sample_num, len(images_files)))\n",
    "    query_file_sample = random.sample(images_files,min(1, len(images_files)))\n",
    "    \n",
    "    for image_file in image_files_sample:\n",
    "        save_emb(emb_dir, image_file)\n",
    "    for image_file in query_file_sample:\n",
    "        save_emb(emb_dir, image_file)\n",
    "    \n",
    "    sku_mean_dict[sku] = cal_sku_embedding_mean(image_files_sample)\n",
    "    db_file_pool.extend(image_files_sample)\n",
    "    query_file_pool.extend(query_file_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_list = []\n",
    "for query_file in query_file_pool:\n",
    "    print \"retrieving for {} ...\".format(query_file)\n",
    "    dist_list = []\n",
    "    sku_truth = query_file.split('/')[-2]\n",
    "    query_file_name = query_file.split('/')[-1]\n",
    "    query_emb = load_emb(emb_dir,query_file)\n",
    "    \n",
    "    for db_file in db_file_pool:\n",
    "        dist = cal_dist(query_file,db_file)\n",
    "        sku = db_file.split('/')[-2]\n",
    "        db_file_name = db_file.split('/')[-1]\n",
    "        dist_sku_mean = np.sqrt(np.sum(np.square(sku_mean_dict[sku]-query_emb)))\n",
    "        dist_list.append([db_file_name, sku, dist, dist_sku_mean])\n",
    "        \n",
    "    dist_df = pd.DataFrame(dist_list, columns=['db_file_name','sku','dist','dist_sku_mean'])\n",
    "    dist_df.to_csv('/home/caffe/facenet_eval/test2test_query_{}_{}.csv'.format(sku_truth, query_file_name),index=False)\n",
    "    closest_sku_by_dist = dist_df[dist_df['dist'] == min(dist_df['dist'])]['sku'].tolist()[0]\n",
    "    closest_sku_by_dist2mean = dist_df[dist_df['dist_sku_mean'] == min(dist_df['dist_sku_mean'])]['sku'].tolist()[0]\n",
    "    retrieval_list.append([query_file_name,sku_truth,closest_sku_by_dist,closest_sku_by_dist2mean])\n",
    "\n",
    "retrieval_df = pd.DataFrame(retrieval_list, \n",
    "                                columns=['query_file_name','sku_truth','closest_sku_by_dist','closest_sku_by_dist2mean'])\n",
    "retrieval_df['by_dist_eval'] = retrieval_df['sku_truth'] == retrieval_df['closest_sku_by_dist']\n",
    "retrieval_df['by_dist2mean_eval'] = retrieval_df['sku_truth'] == retrieval_df['closest_sku_by_dist2mean']\n",
    "by_dist_accuracy = sum(retrieval_df['by_dist_eval'])/len(retrieval_df)*1.0\n",
    "by_dist2mean_accuracy = sum(retrieval_df['by_dist2mean_eval'])./len(retrieval_df)*1.0\n",
    "\n",
    "retrieval_df.to_csv('retrieval_eval.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_dist_accuracy = sum(retrieval_df['by_dist_eval'])*1.0/len(retrieval_df)\n",
    "by_dist2mean_accuracy = sum(retrieval_df['by_dist2mean_eval'])*1.0/len(retrieval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "by_dist_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_dist2mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate the distance to sku mean for each crop and output sorted crops based on distance (intra-cluster distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_image = '/datadrive/images/activelearning/ccna_add_train_reference_crop_test/1018120/tmp#110460.jpg'\n",
    "#query_emb = cal_emb(query_image)\n",
    "image_dir = '/datadrive/images/activelearning/ccna_add_train_reference_crop_test'\n",
    "sorted_image_dir = '/datadrive/images/activelearning/ccna_add_train_reference_crop_test_sorted'\n",
    "sorted_eval = '/home/caffe/face_eval/sorted'\n",
    "emb_dir = '/datadrive/images/activelearning/ccna_add_train_reference_crop_test_emb'\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "sku_mean_dict={}\n",
    "query_file_pool = []\n",
    "\n",
    "for sku_dir in glob.glob(image_dir + '/*'):\n",
    "    sku = sku_dir.split('/')[-1]\n",
    "    images_files = [image_file for image_file in glob.glob(sku_dir + '/*')]\n",
    "    image_files_sample = images_files\n",
    "    query_file_sample = images_files  \n",
    "    for image_file in image_files_sample:\n",
    "        save_emb(emb_dir, image_file)\n",
    "    sku_mean_dict[sku] = cal_sku_embedding_mean(image_files_sample)\n",
    "    query_file_pool.extend(query_file_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_list = []\n",
    "for query_file in query_file_pool:\n",
    "    print \"retrieving for {} ...\".format(query_file)\n",
    "    sku_truth = query_file.split('/')[-2]\n",
    "    query_file_name = query_file.split('/')[-1]\n",
    "    query_emb = load_emb(emb_dir,query_file)\n",
    "    dist_sku_mean = np.sqrt(np.sum(np.square(sku_mean_dict[sku_truth]-query_emb)))\n",
    "    dist_list.append([query_file_name, sku_truth, dist_sku_mean])\n",
    "    #dist_df['sorted_index'] = np.argsort(dist_df['dist_sku_mean'], axis=1)\n",
    "    #query_sorted_index = dist_df\n",
    "    #if not os.path.exists(sorted_image_dir + '/' + sku_truth):\n",
    "    #    os.makedirs(sorted_image_dir + '/' + sku_truth)\n",
    "    \n",
    "    #if not os.path.exists(sorted_eval + '/' + sku_truth):\n",
    "    #    os.makedirse(sorted_eval)                \n",
    "dist_df = pd.DataFrame(dist_list, columns=['query_file_name','sku_truth','dist2sku_mean'])\n",
    "dist_df['sorted_index'] = dist_df.groupby('sku_truth')['dist2sku_mean'].rank(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df.to_csv('sku_intra_cluster_dist.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -R $sorted_image_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in dist_df.iterrows():\n",
    "    source_file = os.path.join(image_dir, row['sku_truth'], row['query_file_name'])\n",
    "    if not os.path.exists(os.path.join(sorted_image_dir, row['sku_truth'])):\n",
    "        os.makedirs(os.path.join(sorted_image_dir, row['sku_truth']))\n",
    "    dest_file = os.path.join(sorted_image_dir, row['sku_truth'], '{:05d}'.format(int(row['sorted_index']))+'_'+row['query_file_name'])\n",
    "    shutil.copy(source_file, dest_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## given a reference image from PMS, calculate its distance from other images in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image = '/datadrive/images/activelearning/ref_pms/1018120/Red Bull Sugarfree16.9.png'\n",
    "image_ref = cv2.imread(ref_image)\n",
    "image_ref = resize_image_to_larger_dimension_and_pad(image_ref, (image_size,image_size), pad_value=255)\n",
    "cv2.imwrite('/datadrive/images/activelearning/ref_pms/1018120/Red Bull Sugarfree16.9_resized.png',image_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image = '/datadrive/images/activelearning/ref_pms/1018120/Red Bull Sugarfree16.9_resized.png'\n",
    "image_dir = '/datadrive/images/activelearning/ccna_add_train_reference_crop_test'\n",
    "sample_num = 5\n",
    "\n",
    "image_file_pool = []\n",
    "for sku_dir in glob.glob(image_dir + '/*'):\n",
    "    images_files = [image_file for image_file in glob.glob(sku_dir + '/*')]\n",
    "    image_file_pool.extend(random.sample(images_files,min(sample_num, len(images_files))))\n",
    "\n",
    "lst = []\n",
    "for image_file in image_file_pool:\n",
    "    dist = cal_dist(ref_image,image_file)\n",
    "    sku = image_file.split('/')[-2]\n",
    "    file_name = image_file.split('/')[-1]\n",
    "    lst.append([file_name, sku, dist])\n",
    "    print file_name,sku,dist\n",
    "\n",
    "df = pd.DataFrame(lst, columns=['file_name','sku','distance'])\n",
    "df.to_csv('ref_test_check.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCCC Evaluation - Distance2Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate embedding and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put X% query images as ref images for training if needed\n",
    "\n",
    "!rm /datadrive/images/activelearning/tccc_train/ -R\n",
    "!rm /datadrive/images/activelearning/tccc_test/ -R\n",
    "\n",
    "query_image_dir = '/datadrive/images/activelearning/tccc_fridge_image_crop'\n",
    "ref_image_dir = '/datadrive/images/activelearning/tccc_reference_image_crop'\n",
    "\n",
    "train_dir = '/datadrive/images/activelearning/tccc_train'\n",
    "test_dir = '/datadrive/images/activelearning/tccc_test'\n",
    "train_emb_dir = '/datadrive/images/activelearning/tccc_train_emb'\n",
    "test_emb_dir = '/datadrive/images/activelearning/tccc_test_emb'\n",
    "\n",
    "sample_perc= 0\n",
    "\n",
    "for sku_dir in glob.glob(ref_image_dir + '/*'):\n",
    "    sku = sku_dir.split('/')[-1]\n",
    "    if not os.path.exists(os.path.join(train_dir, sku)):\n",
    "        os.makedirs(os.path.join(train_dir, sku))         \n",
    "    train_file_sample = [image_file for image_file in glob.glob(sku_dir + '/*')]\n",
    "    for train_file in train_file_sample:\n",
    "        source_file = train_file\n",
    "        dest_file = os.path.join(train_dir, sku, 'train_'+train_file.split('/')[-1].split('.')[0]+'.jpg')\n",
    "        shutil.copy(source_file, dest_file)\n",
    "                          \n",
    "for sku_dir in glob.glob(query_image_dir + '/*'):\n",
    "    sku = sku_dir.split('/')[-1]\n",
    "    if not os.path.exists(os.path.join(train_dir, sku)):\n",
    "        os.makedirs(os.path.join(train_dir, sku))\n",
    "    if not os.path.exists(os.path.join(test_dir, sku)):\n",
    "        os.makedirs(os.path.join(test_dir, sku))\n",
    "    query_file_sample = [image_file for image_file in glob.glob(sku_dir + '/*')]\n",
    "    train_file_sample = random.sample(query_file_sample,min(int(sample_perc*len(query_file_sample)), len(query_file_sample)))\n",
    "    test_file_sample = [image_file for image_file in query_file_sample if image_file not in train_file_sample]\n",
    "    for train_file in train_file_sample:\n",
    "        source_file = train_file\n",
    "        dest_file = os.path.join(train_dir, sku, 'test_'+train_file.split('/')[-1].split('.')[0]+'.jpg')\n",
    "        shutil.copy(source_file, dest_file)\n",
    "    for test_file in test_file_sample:\n",
    "        source_file = test_file\n",
    "        dest_file = os.path.join(test_dir, sku, 'test_'+test_file.split('/')[-1].split('.')[0]+'.jpg')\n",
    "        shutil.copy(source_file, dest_file)\n",
    "                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## generate embedding\n",
    "\n",
    "query_image_dir = '/datadrive/images/activelearning/ccna_add_train_reference_crop_test'\n",
    "ref_image_dir = '/datadrive/images/activelearning/ccna_add_train_reference_crop'\n",
    "query_emb_dir = '/datadrive/images/activelearning/ccna_add_train_reference_crop_test_emb'\n",
    "ref_emb_dir = '/datadrive/images/activelearning/ccna_add_train_reference_crop_emb'\n",
    "\n",
    "#query_image_dir = test_dir\n",
    "#ref_image_dir = train_dir\n",
    "#query_emb_dir = test_emb_dir\n",
    "#ref_emb_dir = train_emb_dir\n",
    "\n",
    "sku_mean_dict={}\n",
    "db_file_pool = []\n",
    "query_file_pool = []\n",
    "\n",
    "for sku_dir in glob.glob(query_image_dir + '/*'):\n",
    "    sku = sku_dir.split('/')[-1]\n",
    "    query_file_sample = [image_file for image_file in glob.glob(sku_dir + '/*')]\n",
    "    for image_file in query_file_sample:\n",
    "        save_emb(query_emb_dir, image_file)\n",
    "    query_file_pool.extend(query_file_sample)\n",
    "    \n",
    "for sku_dir in glob.glob(ref_image_dir + '/*'):\n",
    "    sku = sku_dir.split('/')[-1]\n",
    "    ref_file_sample = [image_file for image_file in glob.glob(sku_dir + '/*')]\n",
    "    for image_file in ref_file_sample:\n",
    "        #print \"saving embedding for {}\".format(image_file)\n",
    "        save_emb(ref_emb_dir, image_file)\n",
    "    db_file_pool.extend(ref_file_sample)\n",
    "    sku_mean_dict[sku] = cal_sku_embedding_mean(ref_emb_dir,ref_file_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calcualte distance to sku mean, classification and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_list = []\n",
    "for query_file in query_file_pool:\n",
    "    print \"retrieving for {} ...\".format(query_file)\n",
    "    dist_list = []\n",
    "    sku_truth = query_file.split('/')[-2]\n",
    "    query_file_name = query_file.split('/')[-1]\n",
    "    query_emb = load_emb(query_emb_dir,query_file)\n",
    "    \n",
    "    for db_file in db_file_pool:\n",
    "        dist = cal_dist(query_emb_dir,query_file,ref_emb_dir,db_file)\n",
    "        sku = db_file.split('/')[-2]\n",
    "        db_file_name = db_file.split('/')[-1]\n",
    "        dist_sku_mean = np.sqrt(np.sum(np.square(sku_mean_dict[sku]-query_emb)))\n",
    "        dist_list.append([db_file_name, sku, dist, dist_sku_mean])\n",
    "        \n",
    "    dist_df = pd.DataFrame(dist_list, columns=['db_file_name','sku','dist','dist_sku_mean'])\n",
    "    #dist_df.to_csv('/home/caffe/facenet_eval/test2test_query_{}_{}.csv'.format(sku_truth, query_file_name),index=False)\n",
    "    closest_sku_by_dist = dist_df[dist_df['dist'] == min(dist_df['dist'])]['sku'].tolist()[0]\n",
    "    closest_sku_by_dist2mean = dist_df[dist_df['dist_sku_mean'] == min(dist_df['dist_sku_mean'])]['sku'].tolist()[0]\n",
    "    retrieval_list.append([query_file_name,sku_truth,closest_sku_by_dist,closest_sku_by_dist2mean])\n",
    "\n",
    "retrieval_df = pd.DataFrame(retrieval_list, \n",
    "                                columns=['query_file_name','sku_truth','closest_sku_by_dist','closest_sku_by_dist2mean'])\n",
    "retrieval_df['by_dist_eval'] = retrieval_df['sku_truth'] == retrieval_df['closest_sku_by_dist']\n",
    "retrieval_df['by_dist2mean_eval'] = retrieval_df['sku_truth'] == retrieval_df['closest_sku_by_dist2mean']\n",
    "by_dist_accuracy = sum(retrieval_df['by_dist_eval'])*1.0/len(retrieval_df)\n",
    "by_dist2mean_accuracy = sum(retrieval_df['by_dist2mean_eval'])*1.0/len(retrieval_df)\n",
    "\n",
    "retrieval_df.to_csv('tccc_classification_eval.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_dist_accuracy = sum(retrieval_df['by_dist_eval'])*1.0/len(retrieval_df)\n",
    "by_dist2mean_accuracy = sum(retrieval_df['by_dist2mean_eval'])*1.0/len(retrieval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_dist_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_dist2mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM/KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC, NuSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array([db_file.split('/')[-2] for db_file in db_file_pool])\n",
    "\n",
    "train_emb = np.zeros((len(db_file_pool), 128))\n",
    "for i,db_file in enumerate(db_file_pool):\n",
    "    train_emb[i] = load_emb(ref_emb_dir,db_file)\n",
    "    \n",
    "test_labels = np.array([query_file.split('/')[-2] for query_file in query_file_pool])\n",
    "\n",
    "test_emb = np.zeros((len(query_file_pool), 128))\n",
    "for i,query_file in enumerate(query_file_pool):\n",
    "    test_emb[i] = load_emb(query_emb_dir,query_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "svc = LinearSVC(C=50,class_weight='balanced',max_iter=100000)\n",
    "#rf = RandomForestClassifier(n_estimators=100)\n",
    "knn.fit(train_emb, train_labels)\n",
    "svc.fit(train_emb, train_labels)\n",
    "#rf.fit(train_emb, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_knn = accuracy_score(test_labels, knn.predict(test_emb))\n",
    "acc_svc = accuracy_score(test_labels, svc.predict(test_emb))\n",
    "#acc_rf = accuracy_score(test_labels, rf.predict(test_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot confusion matrixy\n",
    "pred_labels = knn.predict(test_emb)\n",
    "class_names = set(test_labels)\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(test_labels, pred_labels)\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20), dpi=80)\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20), dpi=80)\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "train_tsne_emb = TSNE(n_components=2).fit_transform(train_emb)\n",
    "test_tsne_emb = TSNE(n_components=2).fit_transform(test_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20), dpi=80)\n",
    "for i, t in enumerate(sorted(list(set(train_labels)))):\n",
    "    idx = train_labels == t\n",
    "    plt.scatter(train_tsne_emb[idx, 0], train_tsne_emb[idx, 1], label=t)\n",
    "    plt.annotate(t, \n",
    "                 train_tsne_emb[idx].mean(axis=0),\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=10, weight='bold',\n",
    "                 label=t) \n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20), dpi=80)\n",
    "for i, t in enumerate(sorted(list(set(test_labels)))):\n",
    "    idx = test_labels == t\n",
    "    plt.scatter(test_tsne_emb[idx, 0], test_tsne_emb[idx, 1], label=t)\n",
    "    plt.annotate(t, \n",
    "                 test_tsne_emb[idx].mean(axis=0),\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=10, weight='bold',\n",
    "                 label=t) \n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca_emb = PCA(n_components=2).fit_transform(train_emb)\n",
    "\n",
    "test_pca_emb = PCA(n_components=2).fit(train_emb).transform(test_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20), dpi=80)\n",
    "for i, t in enumerate(sorted(list(set(train_labels)))):\n",
    "    idx = train_labels == t\n",
    "    plt.scatter(train_pca_emb[idx, 0], train_pca_emb[idx, 1], label=t)\n",
    "    plt.annotate(t, \n",
    "                 train_pca_emb[idx].mean(axis=0),\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=10, weight='bold',\n",
    "                 label=t) \n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20), dpi=80)\n",
    "for i, t in enumerate(sorted(list(set(test_labels)))):\n",
    "    idx = test_labels == t\n",
    "    plt.scatter(test_pca_emb[idx, 0], test_pca_emb[idx, 1], label=t)\n",
    "    plt.annotate(t, \n",
    "                 test_pca_emb[idx].mean(axis=0),\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=10, weight='bold',\n",
    "                 label=t) \n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
